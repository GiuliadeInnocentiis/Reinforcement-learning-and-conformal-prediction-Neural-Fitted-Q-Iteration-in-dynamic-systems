{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiuliadeInnocentiis/Reinforcement-learning-and-conformal-prediction-Neural-Fitted-Q-Iteration-in-dynamic-systems/blob/main/Neural%20Fitted%20Q-Iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceWtX9Nb5pFF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "python_random_seed = 123\n",
        "\n",
        "# Load and prepare data\n",
        "dati = pd.read_csv(\"water_tank.csv\").iloc[:, 1:]\n",
        "lambda_val = 0.2\n",
        "dati['reward_new'] = dati['reward'] - lambda_val * (dati['stato_succ'] - dati['stato'])**2\n",
        "\n",
        "# Split data\n",
        "index = range(7200)\n",
        "training = dati.iloc[index].copy()\n",
        "test = dati.drop(index).copy()\n",
        "\n",
        "# Parameters\n",
        "gamma = 1  # discount factor\n",
        "azioni_possibili = np.arange(0, 10, 0.5)\n",
        "n_iter = 400 # number of FQI iterations\n",
        "\n",
        "# Initialize metrics storage\n",
        "mse = np.zeros(n_iter)\n",
        "media_costo = np.zeros(n_iter)\n",
        "std_costo = np.zeros(n_iter)\n",
        "media_reward = np.zeros(n_iter)\n",
        "std_reward = np.zeros(n_iter)\n",
        "\n",
        "# Define groups for daily aggregation (24 hours)\n",
        "gruppi = np.repeat(np.arange(len(training)//24), 24)\n",
        "\n",
        "# Define neural network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2= nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Prepare data\n",
        "scaler = StandardScaler()\n",
        "X_train = training[['stato', 'azione']].values\n",
        "X_scaled = scaler.fit_transform(X_train)\n",
        "y_train = training['reward_new'].values.reshape(-1, 1)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.FloatTensor(X_scaled)\n",
        "y_tensor = torch.FloatTensor(y_train)\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[1]\n",
        "q_mod = QNetwork(input_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(q_mod.parameters(), lr=0.001)\n",
        "\n",
        "# Define price array\n",
        "prices = np.array([1]*6 + [3]*2 + [2]*10 + [3]*2 + [2]*2 + [1]*2)\n",
        "eta = 0.85\n",
        "\n",
        "# Fitted Q-Iteration\n",
        "# Get all unique states from the entire dataset\n",
        "all_unique_states = pd.concat([training['stato'], training['stato_succ'], test['stato'], test['stato_succ']]).unique()\n",
        "\n",
        "# Create a mapping from state to index\n",
        "state_to_index = {state: i for i, state in enumerate(all_unique_states)}\n",
        "\n",
        "for k in range(n_iter):\n",
        "    print(f\"Iteration FQI: {k+1}/{n_iter}\")\n",
        "\n",
        "    target_q = training['reward_new'].values.copy()\n",
        "\n",
        "    if k > 0:\n",
        "        # Pre-calcolo tutti i Q-values per stati unici\n",
        "        # Process in batches for efficiency\n",
        "        batch_size = 1024\n",
        "        q_next = np.zeros((2, len(all_unique_states)))  # 2 rows: max_q and optimal_action\n",
        "\n",
        "        for i in range(0, len(all_unique_states), batch_size):\n",
        "            batch_states = all_unique_states[i:i+batch_size]\n",
        "\n",
        "            # Create all possible state-action pairs\n",
        "            state_action_pairs = np.array([\n",
        "                [s, a] for s in batch_states for a in azioni_possibili\n",
        "            ])\n",
        "\n",
        "            # Scale and predict\n",
        "            state_action_scaled = scaler.transform(state_action_pairs)\n",
        "            with torch.no_grad():\n",
        "                q_preds = q_mod(torch.FloatTensor(state_action_scaled)).numpy().flatten()\n",
        "                q_preds = q_preds.reshape(len(batch_states), len(azioni_possibili))\n",
        "\n",
        "                # Store max Q and optimal action\n",
        "                q_next[0, i:i+len(batch_states)] = np.max(q_preds, axis=1)\n",
        "                q_next[1, i:i+len(batch_states)] = azioni_possibili[np.argmax(q_preds, axis=1)]\n",
        "\n",
        "        # Update target Q for non-terminal states\n",
        "        not_done = training['done'] != 1\n",
        "        # Use the state_to_index mapping to get the correct index for each successor state\n",
        "        idx = [state_to_index[s] for s in training['stato_succ']]\n",
        "        target_q[not_done] += gamma * q_next[0, idx][not_done]\n",
        "\n",
        "        # Calculate daily costs and rewards\n",
        "        temp_actions = q_next[1, idx]\n",
        "\n",
        "        # Cost calculation\n",
        "        costo_temp = -prices[np.tile(np.arange(24), len(training)//24)] * (1/eta) * (temp_actions**(1/3))\n",
        "        costo_daily = np.array([np.sum(costo_temp[gruppi == g]) for g in np.unique(gruppi)])\n",
        "        media_costo[k] = np.mean(costo_daily)\n",
        "        std_costo[k] = np.std(costo_daily)\n",
        "\n",
        "        # Reward calculation\n",
        "        reward_temp = costo_temp - lambda_val * (training['stato_succ'] - training['stato'])**2\n",
        "        reward_daily = np.array([np.sum(reward_temp[gruppi == g]) for g in np.unique(gruppi)])\n",
        "        media_reward[k] = np.mean(reward_daily)\n",
        "        std_reward[k] = np.std(reward_daily)\n",
        "\n",
        "\n",
        "    # Update y_tensor with new target_q\n",
        "    y_tensor = torch.FloatTensor(target_q.reshape(-1, 1))\n",
        "    dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "    # Train the model\n",
        "    q_mod.train()\n",
        "    for epoch in range(10):\n",
        "        for X_batch, y_batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = q_mod(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Compute MSE\n",
        "    with torch.no_grad():\n",
        "        predictions = q_mod(X_tensor)\n",
        "        mse[k] = criterion(predictions, y_tensor).item()\n",
        "\n",
        "# Optimal action function\n",
        "def optimal_action(state, model, scaler):\n",
        "    state_action_pairs = np.column_stack([\n",
        "        np.full(len(azioni_possibili), state),\n",
        "        azioni_possibili\n",
        "    ])\n",
        "    state_action_scaled = scaler.transform(state_action_pairs)\n",
        "    with torch.no_grad():\n",
        "        q_values = model(torch.FloatTensor(state_action_scaled)).numpy().flatten()\n",
        "    return azioni_possibili[np.argmax(q_values)]\n",
        "\n",
        "# Test the policy\n",
        "test_states = test['stato'].values\n",
        "test_actions = np.array([optimal_action(s, q_mod, scaler) for s in test_states])\n",
        "\n",
        "# Calculate costs\n",
        "test['hour'] = np.tile(np.arange(24), len(test)//24 + 1)[:len(test)] % 24\n",
        "cost = prices[test['hour']] * (1/eta) * (test_actions**(1/3))\n",
        "\n",
        "print(\"Cost difference:\", np.sum(cost + test['reward'].values))\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(mse)\n",
        "plt.title('MSE over Iterations')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('MSE')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(media_costo, label='Mean Cost')\n",
        "plt.plot(media_costo + std_costo, 'r--', alpha=0.3)\n",
        "plt.plot(media_costo - std_costo, 'r--', alpha=0.3)\n",
        "plt.title('Daily Cost over Iterations')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cost')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(media_reward, label='Mean Reward')\n",
        "plt.plot(media_reward + std_reward, 'g--', alpha=0.3)\n",
        "plt.plot(media_reward - std_reward, 'g--', alpha=0.3)\n",
        "plt.title('Daily Reward over Iterations')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Reward')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7trbi5_p72Yi"
      },
      "outputs": [],
      "source": [
        "#Mi salvo i dati che servono per fare grafico reward\n",
        "dati_reward = pd.DataFrame({'media_reward': media_reward, 'std_reward': std_reward, 'iterazione': np.arange(1, n_iter+1), 'lower':media_reward - std_reward, 'upper': media_reward + std_reward})\n",
        "dati_reward.to_csv('dati_reward.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF41mHlA2u0v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Optimal action function\n",
        "def optimal_action(state, model, scaler):\n",
        "    state_action_pairs = np.column_stack([\n",
        "        np.full(len(azioni_possibili), state),\n",
        "        azioni_possibili\n",
        "    ])\n",
        "    state_action_scaled = scaler.transform(state_action_pairs)\n",
        "    with torch.no_grad():\n",
        "        q_values = model(torch.FloatTensor(state_action_scaled)).numpy().flatten()\n",
        "    return azioni_possibili[np.argmax(q_values)]\n",
        "\n",
        "\n",
        "# Calcolo della domanda (equivalente a domanda <- test$stato + test$azione - test$stato_succ)\n",
        "domanda = test['stato'] + test['azione'] - test['stato_succ']\n",
        "\n",
        "# Inizializzazione degli array (equivalente a numeric(nrow(test)))\n",
        "stato_nuovo = np.zeros(len(test)+1 )  # +1 perché salviamo anche lo stato finale\n",
        "stato_nuovo[0] = test['stato'].iloc[0]  # Stato iniziale\n",
        "\n",
        "azione_ottima = np.zeros(len(test))\n",
        "done = test['done'].values\n",
        "stato_oss = test['stato'].values\n",
        "\n",
        "# Simulazione del sistema\n",
        "for i in range(len(test)):\n",
        "    azione_ottima[i] = optimal_action(stato_nuovo[i], q_mod, scaler)\n",
        "    if done[i] == 0:\n",
        "        # Caso non terminale\n",
        "        stato_nuovo[i+1] = stato_nuovo[i] + azione_ottima[i] - domanda.iloc[i]\n",
        "    else:\n",
        "\n",
        "        if i + 1 < len(stato_oss):\n",
        "            stato_nuovo[i+1] = stato_oss[i+1]\n",
        "        else:\n",
        "            # Se i+1 è fuori dai limiti, mantieni l'ultimo stato osservato\n",
        "            stato_nuovo[i+1] = stato_oss[i]\n",
        "\n",
        "# Analisi dei risultati (equivalente ai comandi R finali)\n",
        "stato_nuovo = stato_nuovo[:-1]\n",
        "# Tabella delle azioni ottimali\n",
        "print(\"Distribuzione delle azioni ottimali:\")\n",
        "print(pd.Series(azione_ottima).value_counts().sort_index())\n",
        "\n",
        "# Statistiche dello stato\n",
        "print(\"\\nStatistiche degli stati:\")\n",
        "print(pd.Series(stato_nuovo).describe())\n",
        "\n",
        "# Conteggio stati <= 5 e >= 50\n",
        "print(f\"\\nNumero di stati <= 5: {np.sum(stato_nuovo <= 5)}\")\n",
        "print(f\"Numero di stati >= 50: {np.sum(stato_nuovo >= 50)}\")\n",
        "prices = np.concatenate([\n",
        "    np.ones(6),       # rep(1,6)\n",
        "    np.full(2, 3),    # rep(3,2)\n",
        "    np.full(10, 2),   # rep(2,10)\n",
        "    np.full(2, 3),    # rep(3,2)\n",
        "    np.full(2, 2),    # rep(2,2)\n",
        "    np.full(2, 1)     # rep(1,2)\n",
        "])\n",
        "\n",
        "eta = 0.85\n",
        "\n",
        "# Calcolo del costo (adattato alla tua struttura)\n",
        "# Assumendo che azione_ottima sia un array numpy di lunghezza 24*65=1560\n",
        "costo = np.tile(prices, 65) * (1/eta) * np.power(azione_ottima, 1/3)\n",
        "\n",
        "# Calcolo del costo totale (nota: - -test$reward equivale a + test$reward)\n",
        "# Assumendo che test['reward'] sia una Series pandas o array numpy\n",
        "total_cost = np.sum(costo + test['reward'].values)\n",
        "\n",
        "print(f\"Costo totale: {total_cost}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xxQpkEv3aDR",
        "outputId": "649b7a7f-7507-4187-bc6a-27806aa6f3fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Costo totale: -202.16463515831535\n"
          ]
        }
      ],
      "source": [
        "prices = np.concatenate([\n",
        "    np.ones(6),       # rep(1,6)\n",
        "    np.full(2, 3),    # rep(3,2)\n",
        "    np.full(10, 2),   # rep(2,10)\n",
        "    np.full(2, 3),    # rep(3,2)\n",
        "    np.full(2, 2),    # rep(2,2)\n",
        "    np.full(2, 1)     # rep(1,2)\n",
        "])\n",
        "\n",
        "eta = 0.85\n",
        "\n",
        "# Calcolo del costo (adattato alla tua struttura)\n",
        "# Assumendo che azione_ottima sia un array numpy di lunghezza 24*65=1560\n",
        "costo = np.tile(prices, 65) * (1/eta) * np.power(azione_ottima, 1/3)\n",
        "\n",
        "# Calcolo del costo totale (nota: - -test$reward equivale a + test$reward)\n",
        "# Assumendo che test['reward'] sia una Series pandas o array numpy\n",
        "total_cost = np.sum(costo + test['reward'].values)\n",
        "\n",
        "print(f\"Costo totale: {total_cost}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "19AmsHGvURm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6sp6J46UR3Q"
      },
      "outputs": [],
      "source": [
        "#Faccio su tutto il dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Optimal action function\n",
        "def optimal_action(state, model, scaler):\n",
        "    state_action_pairs = np.column_stack([\n",
        "        np.full(len(azioni_possibili), state),\n",
        "        azioni_possibili\n",
        "    ])\n",
        "    state_action_scaled = scaler.transform(state_action_pairs)\n",
        "    with torch.no_grad():\n",
        "        q_values = model(torch.FloatTensor(state_action_scaled)).numpy().flatten()\n",
        "    return azioni_possibili[np.argmax(q_values)]\n",
        "\n",
        "# Calcolo della domanda (equivalente a domanda <- test$stato + test$azione - test$stato_succ)\n",
        "domanda = dati['stato'] + dati['azione'] - dati['stato_succ']\n",
        "\n",
        "# Inizializzazione degli array (equivalente a numeric(nrow(test)))\n",
        "stato_nuovo = np.zeros(len(dati)+1 )  # +1 perché salviamo anche lo stato finale\n",
        "stato_nuovo[0] = dati['stato'].iloc[0]  # Stato iniziale\n",
        "\n",
        "azione_ottima = np.zeros(len(dati))\n",
        "done = dati['done'].values\n",
        "\n",
        "for i in range(len(dati)):\n",
        "    azione_ottima[i] = optimal_action(stato_nuovo[i], q_mod, scaler)\n",
        "    stato_nuovo[i+1] = stato_nuovo[i] + azione_ottima[i] - domanda.iloc[i]\n",
        "\n",
        "# Analisi dei risultati (equivalente ai comandi R finali)\n",
        "stato_nuovo = stato_nuovo[:-1]\n",
        "# Tabella delle azioni ottimali\n",
        "print(\"Distribuzione delle azioni ottimali:\")\n",
        "print(pd.Series(azione_ottima).value_counts().sort_index())\n",
        "print(pd.Series(azione_ottima).describe())\n",
        "\n",
        "# Statistiche dello stato\n",
        "print(\"\\nStatistiche degli stati:\")\n",
        "print(pd.Series(stato_nuovo).describe())\n",
        "\n",
        "# Conteggio stati <= 5 e >= 50\n",
        "print(f\"\\nNumero di stati <= 5: {np.sum(stato_nuovo <= 5)}\")\n",
        "print(f\"Numero di stati >= 50: {np.sum(stato_nuovo >= 50)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prezzo sul dataset completo\n",
        "prices = np.concatenate([\n",
        "    np.ones(6),       # rep(1,6)\n",
        "    np.full(2, 3),    # rep(3,2)\n",
        "    np.full(10, 2),   # rep(2,10)\n",
        "    np.full(2, 3),    # rep(3,2)\n",
        "    np.full(2, 2),    # rep(2,2)\n",
        "    np.full(2, 1)     # rep(1,2)\n",
        "])\n",
        "\n",
        "eta = 0.85\n",
        "\n",
        "# Calcolo del costo (adattato alla tua struttura)\n",
        "# Assumendo che azione_ottima sia un array numpy di lunghezza 24*65=1560\n",
        "costo = np.tile(prices, 365) * (1/eta) * np.power(azione_ottima, 1/3)\n",
        "\n",
        "# Calcolo del costo totale (nota: - -test$reward equivale a + test$reward)\n",
        "# Assumendo che test['reward'] sia una Series pandas o array numpy\n",
        "total_cost = np.sum(costo + dati['reward'].values)\n",
        "\n",
        "print(f\"Costo totale: {total_cost}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ns_1yMBU0l8",
        "outputId": "b1747e52-39f8-4422-ace6-a5e1a4b3009d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Costo totale: -285.96707664114524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uqetS97rSqt"
      },
      "outputs": [],
      "source": [
        "dati_azione=pd.DataFrame ({'azione' : azione_ottima, 'stato' : stato_nuovo, 'costo' : costo})\n",
        "dati_azione.to_csv('dati_azione_sdg.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM90K96c6JniXa4kEbvxjfR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}