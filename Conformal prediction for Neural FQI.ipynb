{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiuliadeInnocentiis/Reinforcement-learning-and-conformal-prediction-Neural-Fitted-Q-Iteration-in-dynamic-systems/blob/main/Conformal%20prediction%20for%20Neural%20FQI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceWtX9Nb5pFF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "python_random_seed = 123\n",
        "\n",
        "# Load and prepare data\n",
        "dati = pd.read_csv(\"water_tank.csv\").iloc[:, 1:]\n",
        "lambda_val = 0.2\n",
        "dati['reward_new'] = dati['reward'] - lambda_val * (dati['stato_succ'] - dati['stato'])**2\n",
        "\n",
        "# Split data\n",
        "index = range(7200)\n",
        "training = dati.iloc[index].copy()\n",
        "test = dati.drop(index).copy()\n",
        "\n",
        "# Parameters\n",
        "gamma = 1  # discount factor\n",
        "azioni_possibili = np.arange(0, 10, 0.5)\n",
        "n_iter = 400 # number of FQI iterations\n",
        "\n",
        "# Initialize metrics storage\n",
        "mse = np.zeros(n_iter)\n",
        "media_costo = np.zeros(n_iter)\n",
        "std_costo = np.zeros(n_iter)\n",
        "media_reward = np.zeros(n_iter)\n",
        "std_reward = np.zeros(n_iter)\n",
        "\n",
        "# Define groups for daily aggregation (24 hours)\n",
        "gruppi = np.repeat(np.arange(len(training)//24), 24)\n",
        "\n",
        "# Define neural network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2= nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Prepare data\n",
        "scaler = StandardScaler()\n",
        "X_train = training[['stato', 'azione']].values\n",
        "X_scaled = scaler.fit_transform(X_train)\n",
        "y_train = training['reward_new'].values.reshape(-1, 1)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.FloatTensor(X_scaled)\n",
        "y_tensor = torch.FloatTensor(y_train)\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[1]\n",
        "q_mod = QNetwork(input_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(q_mod.parameters(), lr=0.001)\n",
        "\n",
        "# Define price array\n",
        "prices = np.array([1]*6 + [3]*2 + [2]*10 + [3]*2 + [2]*2 + [1]*2)\n",
        "eta = 0.85\n",
        "\n",
        "# Fitted Q-Iteration\n",
        "# Get all unique states from the entire dataset\n",
        "all_unique_states = pd.concat([training['stato'], training['stato_succ'], test['stato'], test['stato_succ']]).unique()\n",
        "\n",
        "# Create a mapping from state to index\n",
        "state_to_index = {state: i for i, state in enumerate(all_unique_states)}\n",
        "\n",
        "for k in range(n_iter):\n",
        "    print(f\"Iteration FQI: {k+1}/{n_iter}\")\n",
        "\n",
        "    target_q = training['reward_new'].values.copy()\n",
        "\n",
        "    if k > 0:\n",
        "        # Pre-calcolo tutti i Q-values per stati unici\n",
        "        # Process in batches for efficiency\n",
        "        batch_size = 1024\n",
        "        q_next = np.zeros((2, len(all_unique_states)))  # 2 rows: max_q and optimal_action\n",
        "\n",
        "        for i in range(0, len(all_unique_states), batch_size):\n",
        "            batch_states = all_unique_states[i:i+batch_size]\n",
        "\n",
        "            # Create all possible state-action pairs\n",
        "            state_action_pairs = np.array([\n",
        "                [s, a] for s in batch_states for a in azioni_possibili\n",
        "            ])\n",
        "\n",
        "            # Scale and predict\n",
        "            state_action_scaled = scaler.transform(state_action_pairs)\n",
        "            with torch.no_grad():\n",
        "                q_preds = q_mod(torch.FloatTensor(state_action_scaled)).numpy().flatten()\n",
        "                q_preds = q_preds.reshape(len(batch_states), len(azioni_possibili))\n",
        "\n",
        "                # Store max Q and optimal action\n",
        "                q_next[0, i:i+len(batch_states)] = np.max(q_preds, axis=1)\n",
        "                q_next[1, i:i+len(batch_states)] = azioni_possibili[np.argmax(q_preds, axis=1)]\n",
        "\n",
        "        # Update target Q for non-terminal states\n",
        "        not_done = training['done'] != 1\n",
        "        # Use the state_to_index mapping to get the correct index for each successor state\n",
        "        idx = [state_to_index[s] for s in training['stato_succ']]\n",
        "        target_q[not_done] += gamma * q_next[0, idx][not_done]\n",
        "\n",
        "        # Calculate daily costs and rewards\n",
        "        temp_actions = q_next[1, idx]\n",
        "\n",
        "        # Cost calculation\n",
        "        costo_temp = -prices[np.tile(np.arange(24), len(training)//24)] * (1/eta) * (temp_actions**(1/3))\n",
        "        costo_daily = np.array([np.sum(costo_temp[gruppi == g]) for g in np.unique(gruppi)])\n",
        "        media_costo[k] = np.mean(costo_daily)\n",
        "        std_costo[k] = np.std(costo_daily)\n",
        "\n",
        "        # Reward calculation\n",
        "        reward_temp = costo_temp - lambda_val * (training['stato_succ'] - training['stato'])**2\n",
        "        reward_daily = np.array([np.sum(reward_temp[gruppi == g]) for g in np.unique(gruppi)])\n",
        "        media_reward[k] = np.mean(reward_daily)\n",
        "        std_reward[k] = np.std(reward_daily)\n",
        "\n",
        "\n",
        "    # Update y_tensor with new target_q\n",
        "    y_tensor = torch.FloatTensor(target_q.reshape(-1, 1))\n",
        "    dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "    # Train the model\n",
        "    q_mod.train()\n",
        "    for epoch in range(10):\n",
        "        for X_batch, y_batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = q_mod(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Compute MSE\n",
        "    with torch.no_grad():\n",
        "        predictions = q_mod(X_tensor)\n",
        "        mse[k] = criterion(predictions, y_tensor).item()\n",
        "\n",
        "# Optimal action function\n",
        "def optimal_action(state, model, scaler):\n",
        "    state_action_pairs = np.column_stack([\n",
        "        np.full(len(azioni_possibili), state),\n",
        "        azioni_possibili\n",
        "    ])\n",
        "    state_action_scaled = scaler.transform(state_action_pairs)\n",
        "    with torch.no_grad():\n",
        "        q_values = model(torch.FloatTensor(state_action_scaled)).numpy().flatten()\n",
        "    return azioni_possibili[np.argmax(q_values)]\n",
        "\n",
        "# Test the policy\n",
        "test_states = test['stato'].values\n",
        "test_actions = np.array([optimal_action(s, q_mod, scaler) for s in test_states])\n",
        "\n",
        "# Calculate costs\n",
        "test['hour'] = np.tile(np.arange(24), len(test)//24 + 1)[:len(test)] % 24\n",
        "cost = prices[test['hour']] * (1/eta) * (test_actions**(1/3))\n",
        "\n",
        "print(\"Cost difference:\", np.sum(cost + test['reward'].values))\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(mse)\n",
        "plt.title('MSE over Iterations')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('MSE')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(media_costo, label='Mean Cost')\n",
        "plt.plot(media_costo + std_costo, 'r--', alpha=0.3)\n",
        "plt.plot(media_costo - std_costo, 'r--', alpha=0.3)\n",
        "plt.title('Daily Cost over Iterations')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cost')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(media_reward, label='Mean Reward')\n",
        "plt.plot(media_reward + std_reward, 'g--', alpha=0.3)\n",
        "plt.plot(media_reward - std_reward, 'g--', alpha=0.3)\n",
        "plt.title('Daily Reward over Iterations')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Reward')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7trbi5_p72Yi"
      },
      "outputs": [],
      "source": [
        "#Mi salvo i dati che servono per fare grafico reward\n",
        "dati_reward = pd.DataFrame({'media_reward': media_reward, 'std_reward': std_reward, 'iterazione': np.arange(1, n_iter+1), 'lower':media_reward - std_reward, 'upper': media_reward + std_reward})\n",
        "dati_reward.to_csv('dati_reward.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF41mHlA2u0v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Optimal action function\n",
        "def optimal_action(state, model, scaler):\n",
        "    state_action_pairs = np.column_stack([\n",
        "        np.full(len(azioni_possibili), state),\n",
        "        azioni_possibili\n",
        "    ])\n",
        "    state_action_scaled = scaler.transform(state_action_pairs)\n",
        "    with torch.no_grad():\n",
        "        q_values = model(torch.FloatTensor(state_action_scaled)).numpy().flatten()\n",
        "    return azioni_possibili[np.argmax(q_values)]\n",
        "\n",
        "\n",
        "# Calcolo della domanda (equivalente a domanda <- test$stato + test$azione - test$stato_succ)\n",
        "domanda = test['stato'] + test['azione'] - test['stato_succ']\n",
        "\n",
        "# Inizializzazione degli array (equivalente a numeric(nrow(test)))\n",
        "stato_nuovo = np.zeros(len(test)+1 )  # +1 perché salviamo anche lo stato finale\n",
        "stato_nuovo[0] = test['stato'].iloc[0]  # Stato iniziale\n",
        "\n",
        "azione_ottima = np.zeros(len(test))\n",
        "done = test['done'].values\n",
        "stato_oss = test['stato'].values\n",
        "\n",
        "# Simulazione del sistema\n",
        "for i in range(len(test)):\n",
        "    azione_ottima[i] = optimal_action(stato_nuovo[i], q_mod, scaler)\n",
        "    if done[i] == 0:\n",
        "        # Caso non terminale\n",
        "        stato_nuovo[i+1] = stato_nuovo[i] + azione_ottima[i] - domanda.iloc[i]\n",
        "    else:\n",
        "\n",
        "        if i + 1 < len(stato_oss):\n",
        "            stato_nuovo[i+1] = stato_oss[i+1]\n",
        "        else:\n",
        "            # Se i+1 è fuori dai limiti, mantieni l'ultimo stato osservato\n",
        "            stato_nuovo[i+1] = stato_oss[i]\n",
        "\n",
        "# Analisi dei risultati (equivalente ai comandi R finali)\n",
        "stato_nuovo = stato_nuovo[:-1]\n",
        "# Tabella delle azioni ottimali\n",
        "print(\"Distribuzione delle azioni ottimali:\")\n",
        "print(pd.Series(azione_ottima).value_counts().sort_index())\n",
        "\n",
        "# Statistiche dello stato\n",
        "print(\"\\nStatistiche degli stati:\")\n",
        "print(pd.Series(stato_nuovo).describe())\n",
        "\n",
        "# Conteggio stati <= 5 e >= 50\n",
        "print(f\"\\nNumero di stati <= 5: {np.sum(stato_nuovo <= 5)}\")\n",
        "print(f\"Numero di stati >= 50: {np.sum(stato_nuovo >= 50)}\")\n",
        "prices = np.concatenate([\n",
        "    np.ones(6),       # rep(1,6)\n",
        "    np.full(2, 3),    # rep(3,2)\n",
        "    np.full(10, 2),   # rep(2,10)\n",
        "    np.full(2, 3),    # rep(3,2)\n",
        "    np.full(2, 2),    # rep(2,2)\n",
        "    np.full(2, 1)     # rep(1,2)\n",
        "])\n",
        "\n",
        "eta = 0.85\n",
        "\n",
        "# Calcolo del costo (adattato alla tua struttura)\n",
        "# Assumendo che azione_ottima sia un array numpy di lunghezza 24*65=1560\n",
        "costo = np.tile(prices, 65) * (1/eta) * np.power(azione_ottima, 1/3)\n",
        "\n",
        "# Calcolo del costo totale (nota: - -test$reward equivale a + test$reward)\n",
        "# Assumendo che test['reward'] sia una Series pandas o array numpy\n",
        "total_cost = np.sum(costo + test['reward'].values)\n",
        "\n",
        "print(f\"Costo totale: {total_cost}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xxQpkEv3aDR"
      },
      "outputs": [],
      "source": [
        "prices = np.concatenate([\n",
        "    np.ones(6),       # rep(1,6)\n",
        "    np.full(2, 3),    # rep(3,2)\n",
        "    np.full(10, 2),   # rep(2,10)\n",
        "    np.full(2, 3),    # rep(3,2)\n",
        "    np.full(2, 2),    # rep(2,2)\n",
        "    np.full(2, 1)     # rep(1,2)\n",
        "])\n",
        "\n",
        "eta = 0.85\n",
        "\n",
        "# Calcolo del costo (adattato alla tua struttura)\n",
        "# Assumendo che azione_ottima sia un array numpy di lunghezza 24*65=1560\n",
        "costo = np.tile(prices, 65) * (1/eta) * np.power(azione_ottima, 1/3)\n",
        "\n",
        "# Calcolo del costo totale (nota: - -test$reward equivale a + test$reward)\n",
        "# Assumendo che test['reward'] sia una Series pandas o array numpy\n",
        "total_cost = np.sum(costo + test['reward'].values)\n",
        "\n",
        "print(f\"Costo totale: {total_cost}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "19AmsHGvURm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6sp6J46UR3Q"
      },
      "outputs": [],
      "source": [
        "#Faccio su tutto il dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Optimal action function\n",
        "def optimal_action(state, model, scaler):\n",
        "    state_action_pairs = np.column_stack([\n",
        "        np.full(len(azioni_possibili), state),\n",
        "        azioni_possibili\n",
        "    ])\n",
        "    state_action_scaled = scaler.transform(state_action_pairs)\n",
        "    with torch.no_grad():\n",
        "        q_values = model(torch.FloatTensor(state_action_scaled)).numpy().flatten()\n",
        "    return azioni_possibili[np.argmax(q_values)]\n",
        "\n",
        "# Calcolo della domanda (equivalente a domanda <- test$stato + test$azione - test$stato_succ)\n",
        "domanda = dati['stato'] + dati['azione'] - dati['stato_succ']\n",
        "\n",
        "# Inizializzazione degli array (equivalente a numeric(nrow(test)))\n",
        "stato_nuovo = np.zeros(len(dati)+1 )  # +1 perché salviamo anche lo stato finale\n",
        "stato_nuovo[0] = dati['stato'].iloc[0]  # Stato iniziale\n",
        "\n",
        "azione_ottima = np.zeros(len(dati))\n",
        "done = dati['done'].values\n",
        "\n",
        "for i in range(len(dati)):\n",
        "    azione_ottima[i] = optimal_action(stato_nuovo[i], q_mod, scaler)\n",
        "    stato_nuovo[i+1] = stato_nuovo[i] + azione_ottima[i] - domanda.iloc[i]\n",
        "\n",
        "# Analisi dei risultati (equivalente ai comandi R finali)\n",
        "stato_nuovo = stato_nuovo[:-1]\n",
        "# Tabella delle azioni ottimali\n",
        "print(\"Distribuzione delle azioni ottimali:\")\n",
        "print(pd.Series(azione_ottima).value_counts().sort_index())\n",
        "print(pd.Series(azione_ottima).describe())\n",
        "\n",
        "# Statistiche dello stato\n",
        "print(\"\\nStatistiche degli stati:\")\n",
        "print(pd.Series(stato_nuovo).describe())\n",
        "\n",
        "# Conteggio stati <= 5 e >= 50\n",
        "print(f\"\\nNumero di stati <= 5: {np.sum(stato_nuovo <= 5)}\")\n",
        "print(f\"Numero di stati >= 50: {np.sum(stato_nuovo >= 50)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prezzo sul dataset completo\n",
        "prices = np.concatenate([\n",
        "    np.ones(6),       # rep(1,6)\n",
        "    np.full(2, 3),    # rep(3,2)\n",
        "    np.full(10, 2),   # rep(2,10)\n",
        "    np.full(2, 3),    # rep(3,2)\n",
        "    np.full(2, 2),    # rep(2,2)\n",
        "    np.full(2, 1)     # rep(1,2)\n",
        "])\n",
        "\n",
        "eta = 0.85\n",
        "\n",
        "# Calcolo del costo (adattato alla tua struttura)\n",
        "# Assumendo che azione_ottima sia un array numpy di lunghezza 24*65=1560\n",
        "costo = np.tile(prices, 365) * (1/eta) * np.power(azione_ottima, 1/3)\n",
        "\n",
        "# Calcolo del costo totale (nota: - -test$reward equivale a + test$reward)\n",
        "# Assumendo che test['reward'] sia una Series pandas o array numpy\n",
        "total_cost = np.sum(costo + dati['reward'].values)\n",
        "\n",
        "print(f\"Costo totale: {total_cost}\")"
      ],
      "metadata": {
        "id": "-ns_1yMBU0l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uqetS97rSqt"
      },
      "outputs": [],
      "source": [
        "dati_azione=pd.DataFrame ({'azione' : azione_ottima, 'stato' : stato_nuovo, 'costo' : costo})\n",
        "dati_azione.to_csv('dati_azione_sdg.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mu0N81WMbbSW",
        "outputId": "acbbf680-21ec-4771-9a9b-053c427721fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration FQI: 1/400\n",
            "Iteration FQI: 2/400\n",
            "Iteration FQI: 3/400\n",
            "Iteration FQI: 4/400\n",
            "Iteration FQI: 5/400\n",
            "Iteration FQI: 6/400\n",
            "Iteration FQI: 7/400\n",
            "Iteration FQI: 8/400\n",
            "Iteration FQI: 9/400\n",
            "Iteration FQI: 10/400\n",
            "Iteration FQI: 11/400\n",
            "Iteration FQI: 12/400\n",
            "Iteration FQI: 13/400\n",
            "Iteration FQI: 14/400\n",
            "Iteration FQI: 15/400\n",
            "Iteration FQI: 16/400\n",
            "Iteration FQI: 17/400\n",
            "Iteration FQI: 18/400\n",
            "Iteration FQI: 19/400\n",
            "Iteration FQI: 20/400\n",
            "Iteration FQI: 21/400\n",
            "Iteration FQI: 22/400\n",
            "Iteration FQI: 23/400\n",
            "Iteration FQI: 24/400\n",
            "Iteration FQI: 25/400\n",
            "Iteration FQI: 26/400\n",
            "Iteration FQI: 27/400\n",
            "Iteration FQI: 28/400\n",
            "Iteration FQI: 29/400\n",
            "Iteration FQI: 30/400\n",
            "Iteration FQI: 31/400\n",
            "Iteration FQI: 32/400\n",
            "Iteration FQI: 33/400\n",
            "Iteration FQI: 34/400\n",
            "Iteration FQI: 35/400\n",
            "Iteration FQI: 36/400\n",
            "Iteration FQI: 37/400\n",
            "Iteration FQI: 38/400\n",
            "Iteration FQI: 39/400\n",
            "Iteration FQI: 40/400\n",
            "Iteration FQI: 41/400\n",
            "Iteration FQI: 42/400\n",
            "Iteration FQI: 43/400\n",
            "Iteration FQI: 44/400\n",
            "Iteration FQI: 45/400\n",
            "Iteration FQI: 46/400\n",
            "Iteration FQI: 47/400\n",
            "Iteration FQI: 48/400\n",
            "Iteration FQI: 49/400\n",
            "Iteration FQI: 50/400\n",
            "Iteration FQI: 51/400\n",
            "Iteration FQI: 52/400\n",
            "Iteration FQI: 53/400\n",
            "Iteration FQI: 54/400\n",
            "Iteration FQI: 55/400\n",
            "Iteration FQI: 56/400\n",
            "Iteration FQI: 57/400\n",
            "Iteration FQI: 58/400\n",
            "Iteration FQI: 59/400\n",
            "Iteration FQI: 60/400\n",
            "Iteration FQI: 61/400\n",
            "Iteration FQI: 62/400\n",
            "Iteration FQI: 63/400\n",
            "Iteration FQI: 64/400\n",
            "Iteration FQI: 65/400\n",
            "Iteration FQI: 66/400\n",
            "Iteration FQI: 67/400\n",
            "Iteration FQI: 68/400\n",
            "Iteration FQI: 69/400\n",
            "Iteration FQI: 70/400\n",
            "Iteration FQI: 71/400\n",
            "Iteration FQI: 72/400\n",
            "Iteration FQI: 73/400\n",
            "Iteration FQI: 74/400\n",
            "Iteration FQI: 75/400\n",
            "Iteration FQI: 76/400\n",
            "Iteration FQI: 77/400\n",
            "Iteration FQI: 78/400\n",
            "Iteration FQI: 79/400\n",
            "Iteration FQI: 80/400\n",
            "Iteration FQI: 81/400\n",
            "Iteration FQI: 82/400\n",
            "Iteration FQI: 83/400\n",
            "Iteration FQI: 84/400\n",
            "Iteration FQI: 85/400\n",
            "Iteration FQI: 86/400\n",
            "Iteration FQI: 87/400\n",
            "Iteration FQI: 88/400\n",
            "Iteration FQI: 89/400\n",
            "Iteration FQI: 90/400\n",
            "Iteration FQI: 91/400\n",
            "Iteration FQI: 92/400\n",
            "Iteration FQI: 93/400\n",
            "Iteration FQI: 94/400\n",
            "Iteration FQI: 95/400\n",
            "Iteration FQI: 96/400\n",
            "Iteration FQI: 97/400\n",
            "Iteration FQI: 98/400\n",
            "Iteration FQI: 99/400\n",
            "Iteration FQI: 100/400\n",
            "Iteration FQI: 101/400\n",
            "Iteration FQI: 102/400\n",
            "Iteration FQI: 103/400\n",
            "Iteration FQI: 104/400\n",
            "Iteration FQI: 105/400\n",
            "Iteration FQI: 106/400\n",
            "Iteration FQI: 107/400\n",
            "Iteration FQI: 108/400\n",
            "Iteration FQI: 109/400\n",
            "Iteration FQI: 110/400\n",
            "Iteration FQI: 111/400\n",
            "Iteration FQI: 112/400\n",
            "Iteration FQI: 113/400\n",
            "Iteration FQI: 114/400\n",
            "Iteration FQI: 115/400\n",
            "Iteration FQI: 116/400\n",
            "Iteration FQI: 117/400\n",
            "Iteration FQI: 118/400\n",
            "Iteration FQI: 119/400\n",
            "Iteration FQI: 120/400\n",
            "Iteration FQI: 121/400\n",
            "Iteration FQI: 122/400\n",
            "Iteration FQI: 123/400\n",
            "Iteration FQI: 124/400\n",
            "Iteration FQI: 125/400\n",
            "Iteration FQI: 126/400\n",
            "Iteration FQI: 127/400\n",
            "Iteration FQI: 128/400\n",
            "Iteration FQI: 129/400\n",
            "Iteration FQI: 130/400\n",
            "Iteration FQI: 131/400\n",
            "Iteration FQI: 132/400\n",
            "Iteration FQI: 133/400\n",
            "Iteration FQI: 134/400\n",
            "Iteration FQI: 135/400\n",
            "Iteration FQI: 136/400\n",
            "Iteration FQI: 137/400\n",
            "Iteration FQI: 138/400\n",
            "Iteration FQI: 139/400\n",
            "Iteration FQI: 140/400\n",
            "Iteration FQI: 141/400\n",
            "Iteration FQI: 142/400\n",
            "Iteration FQI: 143/400\n",
            "Iteration FQI: 144/400\n",
            "Iteration FQI: 145/400\n",
            "Iteration FQI: 146/400\n",
            "Iteration FQI: 147/400\n",
            "Iteration FQI: 148/400\n",
            "Iteration FQI: 149/400\n",
            "Iteration FQI: 150/400\n",
            "Iteration FQI: 151/400\n",
            "Iteration FQI: 152/400\n",
            "Iteration FQI: 153/400\n",
            "Iteration FQI: 154/400\n",
            "Iteration FQI: 155/400\n",
            "Iteration FQI: 156/400\n",
            "Iteration FQI: 157/400\n",
            "Iteration FQI: 158/400\n",
            "Iteration FQI: 159/400\n",
            "Iteration FQI: 160/400\n",
            "Iteration FQI: 161/400\n",
            "Iteration FQI: 162/400\n",
            "Iteration FQI: 163/400\n",
            "Iteration FQI: 164/400\n",
            "Iteration FQI: 165/400\n",
            "Iteration FQI: 166/400\n",
            "Iteration FQI: 167/400\n",
            "Iteration FQI: 168/400\n",
            "Iteration FQI: 169/400\n",
            "Iteration FQI: 170/400\n",
            "Iteration FQI: 171/400\n",
            "Iteration FQI: 172/400\n",
            "Iteration FQI: 173/400\n",
            "Iteration FQI: 174/400\n",
            "Iteration FQI: 175/400\n",
            "Iteration FQI: 176/400\n",
            "Iteration FQI: 177/400\n",
            "Iteration FQI: 178/400\n",
            "Iteration FQI: 179/400\n",
            "Iteration FQI: 180/400\n",
            "Iteration FQI: 181/400\n",
            "Iteration FQI: 182/400\n",
            "Iteration FQI: 183/400\n",
            "Iteration FQI: 184/400\n",
            "Iteration FQI: 185/400\n",
            "Iteration FQI: 186/400\n",
            "Iteration FQI: 187/400\n",
            "Iteration FQI: 188/400\n",
            "Iteration FQI: 189/400\n",
            "Iteration FQI: 190/400\n",
            "Iteration FQI: 191/400\n",
            "Iteration FQI: 192/400\n",
            "Iteration FQI: 193/400\n",
            "Iteration FQI: 194/400\n",
            "Iteration FQI: 195/400\n",
            "Iteration FQI: 196/400\n",
            "Iteration FQI: 197/400\n",
            "Iteration FQI: 198/400\n",
            "Iteration FQI: 199/400\n",
            "Iteration FQI: 200/400\n",
            "Iteration FQI: 201/400\n",
            "Iteration FQI: 202/400\n",
            "Iteration FQI: 203/400\n",
            "Iteration FQI: 204/400\n",
            "Iteration FQI: 205/400\n",
            "Iteration FQI: 206/400\n",
            "Iteration FQI: 207/400\n",
            "Iteration FQI: 208/400\n",
            "Iteration FQI: 209/400\n",
            "Iteration FQI: 210/400\n",
            "Iteration FQI: 211/400\n",
            "Iteration FQI: 212/400\n",
            "Iteration FQI: 213/400\n",
            "Iteration FQI: 214/400\n",
            "Iteration FQI: 215/400\n",
            "Iteration FQI: 216/400\n",
            "Iteration FQI: 217/400\n",
            "Iteration FQI: 218/400\n",
            "Iteration FQI: 219/400\n",
            "Iteration FQI: 220/400\n",
            "Iteration FQI: 221/400\n",
            "Iteration FQI: 222/400\n",
            "Iteration FQI: 223/400\n",
            "Iteration FQI: 224/400\n",
            "Iteration FQI: 225/400\n",
            "Iteration FQI: 226/400\n",
            "Iteration FQI: 227/400\n",
            "Iteration FQI: 228/400\n",
            "Iteration FQI: 229/400\n",
            "Iteration FQI: 230/400\n",
            "Iteration FQI: 231/400\n",
            "Iteration FQI: 232/400\n",
            "Iteration FQI: 233/400\n",
            "Iteration FQI: 234/400\n",
            "Iteration FQI: 235/400\n",
            "Iteration FQI: 236/400\n",
            "Iteration FQI: 237/400\n",
            "Iteration FQI: 238/400\n",
            "Iteration FQI: 239/400\n",
            "Iteration FQI: 240/400\n",
            "Iteration FQI: 241/400\n",
            "Iteration FQI: 242/400\n",
            "Iteration FQI: 243/400\n",
            "Iteration FQI: 244/400\n",
            "Iteration FQI: 245/400\n",
            "Iteration FQI: 246/400\n",
            "Iteration FQI: 247/400\n",
            "Iteration FQI: 248/400\n",
            "Iteration FQI: 249/400\n",
            "Iteration FQI: 250/400\n",
            "Iteration FQI: 251/400\n",
            "Iteration FQI: 252/400\n",
            "Iteration FQI: 253/400\n",
            "Iteration FQI: 254/400\n",
            "Iteration FQI: 255/400\n",
            "Iteration FQI: 256/400\n",
            "Iteration FQI: 257/400\n",
            "Iteration FQI: 258/400\n",
            "Iteration FQI: 259/400\n",
            "Iteration FQI: 260/400\n",
            "Iteration FQI: 261/400\n",
            "Iteration FQI: 262/400\n",
            "Iteration FQI: 263/400\n",
            "Iteration FQI: 264/400\n",
            "Iteration FQI: 265/400\n",
            "Iteration FQI: 266/400\n",
            "Iteration FQI: 267/400\n",
            "Iteration FQI: 268/400\n",
            "Iteration FQI: 269/400\n",
            "Iteration FQI: 270/400\n",
            "Iteration FQI: 271/400\n",
            "Iteration FQI: 272/400\n",
            "Iteration FQI: 273/400\n",
            "Iteration FQI: 274/400\n",
            "Iteration FQI: 275/400\n",
            "Iteration FQI: 276/400\n",
            "Iteration FQI: 277/400\n",
            "Iteration FQI: 278/400\n",
            "Iteration FQI: 279/400\n",
            "Iteration FQI: 280/400\n",
            "Iteration FQI: 281/400\n",
            "Iteration FQI: 282/400\n",
            "Iteration FQI: 283/400\n",
            "Iteration FQI: 284/400\n",
            "Iteration FQI: 285/400\n",
            "Iteration FQI: 286/400\n",
            "Iteration FQI: 287/400\n",
            "Iteration FQI: 288/400\n",
            "Iteration FQI: 289/400\n",
            "Iteration FQI: 290/400\n",
            "Iteration FQI: 291/400\n",
            "Iteration FQI: 292/400\n",
            "Iteration FQI: 293/400\n",
            "Iteration FQI: 294/400\n",
            "Iteration FQI: 295/400\n",
            "Iteration FQI: 296/400\n",
            "Iteration FQI: 297/400\n",
            "Iteration FQI: 298/400\n",
            "Iteration FQI: 299/400\n",
            "Iteration FQI: 300/400\n",
            "Iteration FQI: 301/400\n",
            "Iteration FQI: 302/400\n",
            "Iteration FQI: 303/400\n",
            "Iteration FQI: 304/400\n",
            "Iteration FQI: 305/400\n",
            "Iteration FQI: 306/400\n",
            "Iteration FQI: 307/400\n",
            "Iteration FQI: 308/400\n",
            "Iteration FQI: 309/400\n",
            "Iteration FQI: 310/400\n",
            "Iteration FQI: 311/400\n",
            "Iteration FQI: 312/400\n",
            "Iteration FQI: 313/400\n",
            "Iteration FQI: 314/400\n",
            "Iteration FQI: 315/400\n",
            "Iteration FQI: 316/400\n",
            "Iteration FQI: 317/400\n",
            "Iteration FQI: 318/400\n",
            "Iteration FQI: 319/400\n",
            "Iteration FQI: 320/400\n",
            "Iteration FQI: 321/400\n",
            "Iteration FQI: 322/400\n",
            "Iteration FQI: 323/400\n",
            "Iteration FQI: 324/400\n",
            "Iteration FQI: 325/400\n",
            "Iteration FQI: 326/400\n",
            "Iteration FQI: 327/400\n",
            "Iteration FQI: 328/400\n",
            "Iteration FQI: 329/400\n",
            "Iteration FQI: 330/400\n",
            "Iteration FQI: 331/400\n",
            "Iteration FQI: 332/400\n",
            "Iteration FQI: 333/400\n",
            "Iteration FQI: 334/400\n",
            "Iteration FQI: 335/400\n",
            "Iteration FQI: 336/400\n",
            "Iteration FQI: 337/400\n",
            "Iteration FQI: 338/400\n",
            "Iteration FQI: 339/400\n",
            "Iteration FQI: 340/400\n",
            "Iteration FQI: 341/400\n",
            "Iteration FQI: 342/400\n",
            "Iteration FQI: 343/400\n",
            "Iteration FQI: 344/400\n",
            "Iteration FQI: 345/400\n",
            "Iteration FQI: 346/400\n",
            "Iteration FQI: 347/400\n",
            "Iteration FQI: 348/400\n",
            "Iteration FQI: 349/400\n",
            "Iteration FQI: 350/400\n",
            "Iteration FQI: 351/400\n",
            "Iteration FQI: 352/400\n",
            "Iteration FQI: 353/400\n",
            "Iteration FQI: 354/400\n",
            "Iteration FQI: 355/400\n",
            "Iteration FQI: 356/400\n",
            "Iteration FQI: 357/400\n",
            "Iteration FQI: 358/400\n",
            "Iteration FQI: 359/400\n",
            "Iteration FQI: 360/400\n",
            "Iteration FQI: 361/400\n",
            "Iteration FQI: 362/400\n",
            "Iteration FQI: 363/400\n",
            "Iteration FQI: 364/400\n",
            "Iteration FQI: 365/400\n",
            "Iteration FQI: 366/400\n",
            "Iteration FQI: 367/400\n",
            "Iteration FQI: 368/400\n",
            "Iteration FQI: 369/400\n",
            "Iteration FQI: 370/400\n",
            "Iteration FQI: 371/400\n",
            "Iteration FQI: 372/400\n",
            "Iteration FQI: 373/400\n",
            "Iteration FQI: 374/400\n",
            "Iteration FQI: 375/400\n",
            "Iteration FQI: 376/400\n",
            "Iteration FQI: 377/400\n",
            "Iteration FQI: 378/400\n",
            "Iteration FQI: 379/400\n",
            "Iteration FQI: 380/400\n",
            "Iteration FQI: 381/400\n",
            "Iteration FQI: 382/400\n",
            "Iteration FQI: 383/400\n",
            "Iteration FQI: 384/400\n",
            "Iteration FQI: 385/400\n",
            "Iteration FQI: 386/400\n",
            "Iteration FQI: 387/400\n",
            "Iteration FQI: 388/400\n",
            "Iteration FQI: 389/400\n",
            "Iteration FQI: 390/400\n",
            "Iteration FQI: 391/400\n",
            "Iteration FQI: 392/400\n",
            "Iteration FQI: 393/400\n",
            "Iteration FQI: 394/400\n",
            "Iteration FQI: 395/400\n",
            "Iteration FQI: 396/400\n",
            "Iteration FQI: 397/400\n",
            "Iteration FQI: 398/400\n",
            "Iteration FQI: 399/400\n",
            "Iteration FQI: 400/400\n"
          ]
        }
      ],
      "source": [
        "#CODICE PER CONFORMAL PREDICTION\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "python_random_seed = 123\n",
        "\n",
        "# Load and prepare data\n",
        "dati = pd.read_csv(\"water_tank.csv\").iloc[:, 1:]\n",
        "lambda_val = 0.2\n",
        "dati['reward_new'] = dati['reward'] - lambda_val * (dati['stato_succ'] - dati['stato'])**2\n",
        "\n",
        "#aggiungo il tempo\n",
        "tempo = np.tile(np.arange(1, 25), len(dati)//24)\n",
        "dati['tempo'] = tempo[:len(dati)]\n",
        "# Split data\n",
        "#Conformal prediction\n",
        "#divido in training, calibration e test\n",
        "index = range(7200)\n",
        "training = dati.iloc[index].copy()\n",
        "test = dati.drop(index).copy()\n",
        "\n",
        "#divido in calibration\n",
        "idx=range(6120)\n",
        "train_dd=training.iloc[idx].copy()\n",
        "cal = training.drop(idx).copy()\n",
        "\n",
        "# Parameters\n",
        "gamma = 1  # discount factor\n",
        "azioni_possibili = np.arange(0, 10, 0.5)\n",
        "n_iter = 400 # number of FQI iterations\n",
        "\n",
        "\n",
        "\n",
        "# Define groups for daily aggregation (24 hours)\n",
        "gruppi = np.repeat(np.arange(len(train_dd)//24), 24)\n",
        "\n",
        "# Define neural network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Prepare data\n",
        "scaler = StandardScaler()\n",
        "X_train = train_dd[['stato', 'azione']].values\n",
        "X_scaled = scaler.fit_transform(X_train)\n",
        "y_train = train_dd['reward_new'].values.reshape(-1, 1)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.FloatTensor(X_scaled)\n",
        "y_tensor = torch.FloatTensor(y_train)\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[1]\n",
        "q_mod = QNetwork(input_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(q_mod.parameters(), lr=0.001)\n",
        "\n",
        "# Define price array\n",
        "prices = np.array([1]*6 + [3]*2 + [2]*10 + [3]*2 + [2]*2 + [1]*2)\n",
        "eta = 0.85\n",
        "\n",
        "# Fitted Q-Iteration\n",
        "# Get all unique states from the entire dataset\n",
        "all_unique_states = pd.concat([training['stato'], training['stato_succ'], test['stato'], test['stato_succ']]).unique()\n",
        "\n",
        "# Create a mapping from state to index\n",
        "state_to_index = {state: i for i, state in enumerate(all_unique_states)}\n",
        "\n",
        "for k in range(n_iter):\n",
        "    print(f\"Iteration FQI: {k+1}/{n_iter}\")\n",
        "\n",
        "    target_q = train_dd['reward_new'].values.copy()\n",
        "\n",
        "    if k > 0:\n",
        "        # Pre-calcolo tutti i Q-values per stati unici\n",
        "        # Process in batches for efficiency\n",
        "        batch_size = 1024\n",
        "        q_next = np.zeros((2, len(all_unique_states)))  # 2 rows: max_q and optimal_action\n",
        "\n",
        "        for i in range(0, len(all_unique_states), batch_size):\n",
        "            batch_states = all_unique_states[i:i+batch_size]\n",
        "\n",
        "            # Create all possible state-action pairs\n",
        "            state_action_pairs = np.array([\n",
        "                [s, a] for s in batch_states for a in azioni_possibili\n",
        "            ])\n",
        "\n",
        "            # Scale and predict\n",
        "            state_action_scaled = scaler.transform(state_action_pairs)\n",
        "            with torch.no_grad():\n",
        "                q_preds = q_mod(torch.FloatTensor(state_action_scaled)).numpy().flatten()\n",
        "                q_preds = q_preds.reshape(len(batch_states), len(azioni_possibili))\n",
        "\n",
        "                # Store max Q and optimal action\n",
        "                q_next[0, i:i+len(batch_states)] = np.max(q_preds, axis=1)\n",
        "                q_next[1, i:i+len(batch_states)] = azioni_possibili[np.argmax(q_preds, axis=1)]\n",
        "\n",
        "        # Update target Q for non-terminal states\n",
        "        not_done = train_dd['done'] != 1\n",
        "        # Use the state_to_index mapping to get the correct index for each successor state\n",
        "        idx = [state_to_index[s] for s in train_dd['stato_succ']]\n",
        "        target_q[not_done] += gamma * q_next[0, idx][not_done]\n",
        "\n",
        "        # Calculate daily costs and rewards\n",
        "        temp_actions = q_next[1, idx]\n",
        "\n",
        "\n",
        "    # Update y_tensor with new target_q\n",
        "    y_tensor = torch.FloatTensor(target_q.reshape(-1, 1))\n",
        "    dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "    # Train the model\n",
        "    q_mod.train()\n",
        "    for epoch in range(10):\n",
        "        for X_batch, y_batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = q_mod(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Compute MSE\n",
        "    with torch.no_grad():\n",
        "        predictions = q_mod(X_tensor)\n",
        "        mse[k] = criterion(predictions, y_tensor).item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD7dT7vyrmnU",
        "outputId": "7e72b8cb-85f8-4e74-ef21-b2e3fabc96da",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conformal prediction completed!\n",
            "Prediction interval half-width: 119.09460802205552\n",
            "   q_values    PI_lower    PI_upper\n",
            "0 -2.195060 -121.289668  116.899548\n",
            "1 -2.461734 -121.556342  116.632874\n",
            "2 -2.500282 -121.594890  116.594326\n",
            "3 -2.543557 -121.638165  116.551051\n",
            "4 -2.461254 -121.555862  116.633354\n"
          ]
        }
      ],
      "source": [
        "# Optimal action function\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "python_random_seed = 123\n",
        "\n",
        "\n",
        "giorno = np.repeat(np.arange(len(cal)//24), 24)\n",
        "cal = cal.copy()\n",
        "cal['giorno'] = giorno[:len(cal)]\n",
        "\n",
        "\n",
        "\n",
        "def optimal_action(state, model, scaler):\n",
        "    state_action_pairs = np.column_stack([\n",
        "        np.full(len(azioni_possibili), state),\n",
        "        azioni_possibili\n",
        "    ])\n",
        "    state_action_scaled = scaler.transform(state_action_pairs)\n",
        "    with torch.no_grad():\n",
        "        q_values = model(torch.FloatTensor(state_action_scaled)).numpy().flatten()\n",
        "    return azioni_possibili[np.argmax(q_values)]\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Conformal Prediction\n",
        "# Create dataset with observed and optimal actions DAL TRAIN\n",
        "indice_oss = range(480)\n",
        "\n",
        "azione_behav = train_dd['azione'].iloc[indice_oss].values\n",
        "stato_behav = train_dd['stato'].iloc[indice_oss].values\n",
        "statosucc_behav = train_dd['stato_succ'].iloc[indice_oss].values\n",
        "done_behav = train_dd['done'].iloc[indice_oss].values\n",
        "\n",
        "# Update state with new policy\n",
        "domanda = stato_behav + azione_behav - statosucc_behav\n",
        "\n",
        "stato_nuovo_b = np.zeros(len(stato_behav) + 1)\n",
        "stato_nuovo_b[0] = stato_behav[0]\n",
        "azione_ottima_t = np.zeros(len(stato_behav))\n",
        "done = done_behav\n",
        "stato_oss = stato_behav\n",
        "\n",
        "for i in range(len(stato_behav)):\n",
        "    if done[i] == 0:\n",
        "        azione_ottima_t[i] = optimal_action(stato_nuovo_b[i], q_mod, scaler)\n",
        "        stato_nuovo_b[i+1] = stato_nuovo_b[i] + azione_ottima_t[i] - domanda[i]\n",
        "    else:\n",
        "        azione_ottima_t[i] = optimal_action(stato_nuovo_b[i], q_mod, scaler)\n",
        "        if i + 1 < len(stato_oss):\n",
        "            stato_nuovo_b[i+1] = stato_oss[i+1]\n",
        "        else:\n",
        "            stato_nuovo_b[i+1] = stato_oss[i]\n",
        "\n",
        "# Logistic regression for weights\n",
        "dd = pd.DataFrame({\n",
        "    'y': np.concatenate([np.ones(480), np.zeros(480)]),\n",
        "    'azioni': np.concatenate([azione_behav, azione_ottima_t])\n",
        "})\n",
        "dd['azioni'] = np.round(dd['azioni'] * 2) / 2  # Round to nearest 0.5\n",
        "\n",
        "mod_log = LogisticRegression()\n",
        "mod_log.fit(dd[['azioni']], dd['y'])\n",
        "\n",
        "# Optimal action for calibration with Q-values\n",
        "# Update calibration set with new policy\n",
        "\n",
        "def optimal_action_cal(state, model, scaler):\n",
        "    state_action_pairs = np.column_stack([\n",
        "        np.full(len(azioni_possibili), state),\n",
        "        azioni_possibili\n",
        "    ])\n",
        "    state_action_scaled = scaler.transform(state_action_pairs)\n",
        "    with torch.no_grad():\n",
        "        q_values = model(torch.FloatTensor(state_action_scaled)).numpy().flatten()\n",
        "    return np.max(q_values), azioni_possibili[np.argmax(q_values)]\n",
        "\n",
        "\n",
        "\n",
        "domanda_cal = cal['stato'] + cal['azione'] - cal['stato_succ']\n",
        "stato_nuovo_c = np.zeros(len(cal) + 1)\n",
        "stato_nuovo_c[0] = cal['stato'].iloc[0]\n",
        "azione_ottima_c = np.zeros(len(cal))\n",
        "q_val_cal = np.zeros(len(cal))\n",
        "\n",
        "for i in range(len(cal)):\n",
        "    if cal['done'].iloc[i] == 0:\n",
        "        q_val_cal[i], azione_ottima_c[i] = optimal_action_cal(stato_nuovo_c[i], q_mod, scaler)\n",
        "        stato_nuovo_c[i+1] = stato_nuovo_c[i] + azione_ottima_c[i] - domanda_cal.iloc[i]\n",
        "    else:\n",
        "        q_val_cal[i], azione_ottima_c[i] = optimal_action_cal(stato_nuovo_c[i], q_mod, scaler)\n",
        "        if i + 1 < len(cal):\n",
        "            stato_nuovo_c[i+1] = cal['stato'].iloc[i+1]\n",
        "        else:\n",
        "            stato_nuovo_c[i+1] = cal['stato'].iloc[i]\n",
        "\n",
        "# Calculate weights\n",
        "pesi = mod_log.predict_proba(azione_ottima_c.reshape(-1, 1))[:, 1]\n",
        "pesi_norm = pesi / (1 - pesi)\n",
        "\n",
        "# Calculate scores\n",
        "tempo = np.tile(np.arange(1, 25), len(cal)//24)\n",
        "giorno = np.repeat(np.arange(len(cal)//24), 24)\n",
        "cal = cal.copy()\n",
        "cal['tempo'] = tempo[:len(cal)]\n",
        "cal['giorno'] = giorno[:len(cal)]\n",
        "\n",
        "q_val_cal_m = q_val_cal.reshape(len(cal)//24, 24)\n",
        "score = np.full((len(cal)//24, 24), np.nan)\n",
        "\n",
        "for g in range(len(cal)//24):\n",
        "    for t in range(24):\n",
        "       score[g, t] = abs(np.sum(cal.loc[cal['giorno'] == g, 'reward_new'].iloc[t:]) - q_val_cal_m[g, t])\n",
        "\n",
        "score = score.flatten()\n",
        "\n",
        "score_pesati = score* pesi_norm\n",
        "ord_score = np.sort(score_pesati)\n",
        "n = len(cal)\n",
        "q_val = np.ceil((n + 1) * (1 - 0.1)).astype(int)\n",
        "eq = ord_score[q_val - 1]  # -1 because Python is 0-indexed\n",
        "\n",
        "\n",
        "domanda_test = test['stato'] + test['azione'] - test['stato_succ']\n",
        "stato_nuovo_test = np.zeros(len(test) + 1)\n",
        "stato_nuovo_test = test['stato'].iloc()\n",
        "azione_ottima_test = np.zeros(len(test))\n",
        "q_val_test = np.zeros(len(test))\n",
        "\n",
        "for i in range(len(test)):\n",
        "    if test['done'].iloc[i] == 0:\n",
        "        q_val_test[i], azione_ottima_test[i] = optimal_action_cal(stato_nuovo_test[i], q_mod, scaler)\n",
        "        #stato_nuovo_test[i+1] = stato_nuovo_test[i] + azione_ottima_test[i] - domanda_test.iloc[i]\n",
        "    else:\n",
        "        q_val_test[i], azione_ottima_test[i] = optimal_action_cal(stato_nuovo_test[i], q_mod, scaler)\n",
        "\n",
        "\n",
        "# Prediction intervals\n",
        "PI = pd.DataFrame({\n",
        "    'q_values': q_val_test,\n",
        "    'PI_lower': q_val_test - eq,\n",
        "    'PI_upper': q_val_test + eq\n",
        "})\n",
        "\n",
        "print(\"Conformal prediction completed!\")\n",
        "print(f\"Prediction interval half-width: {eq}\")\n",
        "print(PI.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Coverage\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assumendo che 'test' sia un DataFrame pandas\n",
        "n = len(test)\n",
        "giorni = np.repeat(np.arange(1, n//24 + 1), 24)\n",
        "test['giorno'] = giorni\n",
        "\n",
        "reward_cum = []\n",
        "score = []\n",
        "\n",
        "for j in np.unique(giorni):\n",
        "    reward_test = test.loc[test['giorno'] == j, 'reward_new'].values\n",
        "    score_temp = []\n",
        "\n",
        "    for i in range(len(reward_test)):\n",
        "        # Somma cumulativa dalla posizione i alla fine\n",
        "        score_temp.append(np.sum(reward_test[i:]))\n",
        "\n",
        "    reward_cum.extend(score_temp)\n",
        "    score = []  # Re-inizializza score per il prossimo giorno\n",
        "\n",
        "# Converti reward_cum in array numpy per facilità di accesso\n",
        "reward_cum = np.array(reward_cum)\n",
        "\n",
        "\n",
        "\n",
        "coverage = np.mean((reward_cum < PI['PI_upper']) & (reward_cum > PI['PI_lower']))\n",
        "int_len = round(np.mean(PI['PI_upper'] - PI['PI_lower']), 5)\n",
        "\n",
        "print(f\"Coverage: {coverage}\")\n",
        "print(f\"Lunghezza intervallo medio: {int_len}\")"
      ],
      "metadata": {
        "id": "MsMDUX109tou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3ZrlBApjc1b"
      },
      "outputs": [],
      "source": [
        "print(eq)\n",
        "print(PI.describe())\n",
        "PI.to_csv('prediction_intervals_baseline.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(PI)"
      ],
      "metadata": {
        "id": "Preq11zTFjlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "varQyGmbBb37",
        "outputId": "6758e950-cc3f-4ce9-b4c8-11d539a6abe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conformal prediction completed!\n",
            "Prediction interval half-width: 55.80965009833277\n",
            "   q_values   PI_lower   PI_upper\n",
            "0 -2.212772 -58.022422  53.596878\n",
            "1 -2.496914 -58.306564  53.312736\n",
            "2 -2.536345 -58.345995  53.273305\n",
            "3 -2.580882 -58.390532  53.228769\n",
            "4 -2.480922 -58.290572  53.328728\n"
          ]
        }
      ],
      "source": [
        "#CON SUBSAMPLE\n",
        "\n",
        "# Optimal action function\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "python_random_seed = 123\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "idx=range(6120)\n",
        "train_dd=training.iloc[idx].copy()\n",
        "cal = training.drop(idx).copy()\n",
        "tempo = np.tile(np.arange(1, 25), len(cal)//24)\n",
        "giorno = np.repeat(np.arange(len(cal)//24), 24)\n",
        "cal = cal.copy()\n",
        "cal['tempo'] = tempo[:len(cal)]\n",
        "cal['giorno'] = giorno[:len(cal)]\n",
        "\n",
        "# Se 'cal' è un DataFrame pandas\n",
        "ind = np.random.choice(1080, 480, replace=False)\n",
        "ind = np.sort(ind)\n",
        "cal = cal.iloc[ind, :]\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Conformal Prediction\n",
        "# Create dataset with observed and optimal actions DAL TRAIN\n",
        "indice_oss = range(480)\n",
        "\n",
        "azione_behav = train_dd['azione'].iloc[indice_oss].values\n",
        "stato_behav = train_dd['stato'].iloc[indice_oss].values\n",
        "statosucc_behav = train_dd['stato_succ'].iloc[indice_oss].values\n",
        "done_behav = train_dd['done'].iloc[indice_oss].values\n",
        "\n",
        "# Update state with new policy\n",
        "domanda = stato_behav + azione_behav - statosucc_behav\n",
        "\n",
        "stato_nuovo_b = np.zeros(len(stato_behav) + 1)\n",
        "stato_nuovo_b[0] = stato_behav[0]\n",
        "azione_ottima_t = np.zeros(len(stato_behav))\n",
        "done = done_behav\n",
        "stato_oss = stato_behav\n",
        "\n",
        "for i in range(len(stato_behav)):\n",
        "    if done[i] == 0:\n",
        "        azione_ottima_t[i] = optimal_action(stato_nuovo_b[i], q_mod, scaler)\n",
        "        stato_nuovo_b[i+1] = stato_nuovo_b[i] + azione_ottima_t[i] - domanda[i]\n",
        "    else:\n",
        "        azione_ottima_t[i] = optimal_action(stato_nuovo_b[i], q_mod, scaler)\n",
        "        if i + 1 < len(stato_oss):\n",
        "            stato_nuovo_b[i+1] = stato_oss[i+1]\n",
        "        else:\n",
        "            stato_nuovo_b[i+1] = stato_oss[i]\n",
        "\n",
        "# Logistic regression for weights\n",
        "dd = pd.DataFrame({\n",
        "    'y': np.concatenate([np.ones(480), np.zeros(480)]),\n",
        "    'azioni': np.concatenate([azione_behav, azione_ottima_t])\n",
        "})\n",
        "dd['azioni'] = np.round(dd['azioni'] * 2) / 2  # Round to nearest 0.5\n",
        "\n",
        "mod_log = LogisticRegression()\n",
        "mod_log.fit(dd[['azioni']], dd['y'])\n",
        "\n",
        "# Optimal action for calibration with Q-values\n",
        "# Update calibration set with new policy\n",
        "\n",
        "def optimal_action_cal(state, model, scaler):\n",
        "    state_action_pairs = np.column_stack([\n",
        "        np.full(len(azioni_possibili), state),\n",
        "        azioni_possibili\n",
        "    ])\n",
        "    state_action_scaled = scaler.transform(state_action_pairs)\n",
        "    with torch.no_grad():\n",
        "        q_values = model(torch.FloatTensor(state_action_scaled)).numpy().flatten()\n",
        "    return np.max(q_values), azioni_possibili[np.argmax(q_values)]\n",
        "\n",
        "\n",
        "\n",
        "domanda_cal = cal['stato'] + cal['azione'] - cal['stato_succ']\n",
        "stato_nuovo_c = np.zeros(len(cal) + 1)\n",
        "stato_nuovo_c[0] = cal['stato'].iloc[0]\n",
        "azione_ottima_c = np.zeros(len(cal))\n",
        "q_val_cal = np.zeros(len(cal))\n",
        "\n",
        "for i in range(len(cal)):\n",
        "    if cal['done'].iloc[i] == 0:\n",
        "        q_val_cal[i], azione_ottima_c[i] = optimal_action_cal(stato_nuovo_c[i], q_mod, scaler)\n",
        "        stato_nuovo_c[i+1] = stato_nuovo_c[i] + azione_ottima_c[i] - domanda_cal.iloc[i]\n",
        "    else:\n",
        "        q_val_cal[i], azione_ottima_c[i] = optimal_action_cal(stato_nuovo_c[i], q_mod, scaler)\n",
        "        if i + 1 < len(cal):\n",
        "            stato_nuovo_c[i+1] = cal['stato'].iloc[i+1]\n",
        "        else:\n",
        "            stato_nuovo_c[i+1] = cal['stato'].iloc[i]\n",
        "\n",
        "# Calculate weights\n",
        "pesi = mod_log.predict_proba(azione_ottima_c.reshape(-1, 1))[:, 1]\n",
        "pesi_norm = pesi / (1 - pesi)\n",
        "\n",
        "# Calculate scores\n",
        "#tempo = np.tile(np.arange(1, 25), len(cal)//24)\n",
        "#giorno = np.repeat(np.arange(len(cal)//24), 24)\n",
        "#cal = cal.copy()\n",
        "#cal['tempo'] = tempo[:len(cal)]\n",
        "#cal['giorno'] = giorno[:len(cal)]\n",
        "\n",
        "#q_val_cal_m = q_val_cal.reshape(len(cal)//24, 24)\n",
        "#score = np.full((len(cal)//24, 24), np.nan)\n",
        "\n",
        "#for g in range(len(cal)//24):\n",
        "  #  for t in range(24):\n",
        " #       score[g, t] = np.sum(cal.loc[cal['giorno'] == g, 'reward_new'].iloc[t:]) - q_val_cal_m[g, t]\n",
        "\n",
        "#score = score.flatten()\n",
        "\n",
        "\n",
        "# Supponendo che 'cal' sia un DataFrame pandas\n",
        "giorni = cal['giorno'].unique()\n",
        "score = []\n",
        "score1 = []\n",
        "\n",
        "for j in giorni:\n",
        "    # Filtra i reward per il giorno corrente\n",
        "    reward_cal = cal.loc[cal['giorno'] == j, 'reward_new'].values\n",
        "      # Assumendo che q_val esista\n",
        "\n",
        "    for i in range(len(reward_cal)):\n",
        "        # Calcola la somma cumulativa dalla posizione i alla fine\n",
        "        cumulative_sum = np.sum(reward_cal[i:])\n",
        "        # Sottrae il valore q_val corrispondente\n",
        "        score.append(abs(cumulative_sum - q_val_cal[i]))\n",
        "\n",
        "    # Aggiunge i risultati al punteggio totale\n",
        "    score1.extend(score)\n",
        "    score = []  # Resetta la lista per il prossimo giorno\n",
        "\n",
        "# Converti in array numpy se necessario\n",
        "score1 = np.array(score1)\n",
        "\n",
        "score_pesati = score1 * pesi_norm\n",
        "ord_score = np.sort(score_pesati)\n",
        "n = len(cal)\n",
        "q_val = np.ceil((n + 1) * (1 - 0.1)).astype(int)\n",
        "eq = ord_score[q_val - 1]  # -1 because Python is 0-indexed\n",
        "#\n",
        "\n",
        "domanda_test = test['stato'] + test['azione'] - test['stato_succ']\n",
        "#stato_nuovo_test = np.zeros(len(test) + 1)\n",
        "stato_nuovo_test = test['stato'].iloc()\n",
        "azione_ottima_test = np.zeros(len(test))\n",
        "q_val_test = np.zeros(len(test))\n",
        "\n",
        "for i in range(len(test)):\n",
        "    if test['done'].iloc[i] == 0:\n",
        "        q_val_test[i], azione_ottima_test[i] = optimal_action_cal(stato_nuovo_test[i], q_mod, scaler)\n",
        "        #stato_nuovo_test[i+1] = stato_nuovo_test[i] + azione_ottima_test[i] - domanda_test.iloc[i]\n",
        "    else:\n",
        "        q_val_test[i], azione_ottima_test[i] = optimal_action_cal(stato_nuovo_test[i], q_mod, scaler)\n",
        "\n",
        "\n",
        "# Prediction intervals\n",
        "PI = pd.DataFrame({\n",
        "    'q_values': q_val_test,\n",
        "    'PI_lower': q_val_test - eq,\n",
        "    'PI_upper': q_val_test + eq\n",
        "})\n",
        "\n",
        "print(\"Conformal prediction completed!\")\n",
        "print(f\"Prediction interval half-width: {eq}\")\n",
        "print(PI.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CJXW10yUHTxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coverage\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assumendo che 'test' sia un DataFrame pandas\n",
        "n = len(test)\n",
        "giorni = np.repeat(np.arange(1, n//24 + 1), 24)\n",
        "test['giorno'] = giorni\n",
        "\n",
        "reward_cum = []\n",
        "score = []\n",
        "\n",
        "for j in np.unique(giorni):\n",
        "    reward_test = test.loc[test['giorno'] == j, 'reward_new'].values\n",
        "    score_temp = []\n",
        "\n",
        "    for i in range(len(reward_test)):\n",
        "        # Somma cumulativa dalla posizione i alla fine\n",
        "        score_temp.append(np.sum(reward_test[i:]))\n",
        "\n",
        "    reward_cum.extend(score_temp)\n",
        "    score = []  # Re-inizializza score per il prossimo giorno\n",
        "\n",
        "# Converti reward_cum in array numpy per facilità di accesso\n",
        "reward_cum = np.array(reward_cum)\n",
        "\n",
        "# Le ultime righe del codice originale\n",
        "print(\"reward_cum[1:24]:\", reward_cum[:24])\n",
        "print(\"PI$q_values[1:24]:\", PI['q_values'][:24])\n",
        "\n",
        "coverage = np.mean((reward_cum < PI['PI_upper']) & (reward_cum > PI['PI_lower']))\n",
        "int_len = round(np.mean(PI['PI_upper'] - PI['PI_lower']), 5)\n",
        "\n",
        "print(f\"Coverage: {coverage}\")\n",
        "print(f\"Lunghezza intervallo medio: {int_len}\")"
      ],
      "metadata": {
        "id": "z2JI9ZvnHUBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "raJ0U17GHg2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IxkVvlbHhDX"
      },
      "outputs": [],
      "source": [
        "print(eq)\n",
        "print(PI.describe())\n",
        "PI.to_csv('prediction_intervals_base_subsample.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIxO5kSfkbK5TVwQFSCDli",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}